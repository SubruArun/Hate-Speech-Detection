{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All Deep Models",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMNbaWyB6Sk1"
      },
      "source": [
        "**1D-CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pxveNoN6WDT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#reading dataset\n",
        "df=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/naive inp.csv')\n",
        "\n",
        "import keras\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 100\n",
        "#tokenizing data\n",
        "tokenizer =keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['tweet'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "#sequence creation\n",
        "X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
        "X =keras.preprocessing.sequence. pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)\n",
        "\n",
        "#one hot\n",
        "Y = pd.get_dummies(df['class']).values\n",
        "print('Shape of label tensor:', Y.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#splitting data to test and train\n",
        "X_train, X_test, Y_train, Y_test =train_test_split(X,Y, test_size = 0.30, random_state = 4)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Embedding,SpatialDropout1D,LSTM,Bidirectional\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.callbacks import  EarlyStopping\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "#model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "model.add(Conv1D(128, 5, activation='relu')) \n",
        "model.add(Conv1D(128, 5, activation='relu')) \n",
        "model.add(MaxPooling1D()) \n",
        "#model.add(layers.GlobalMaxPooling1D()) \n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "#model.add(50, activation='relu')\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 400\n",
        "\n",
        "#training\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)#,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
        "\n",
        "one_y = Y_train.argmax(-1)\n",
        "\n",
        "#predicting and Model Validation on train data set\n",
        "pred = model.predict_classes(X_train, verbose=1)\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "print(confusion_matrix(one_y,pred))\n",
        "print(classification_report(one_y,pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRf5uVuG7Ii1"
      },
      "source": [
        "**BI-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aF321pY7K-r"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#reading dataset\n",
        "df=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/naive inp.csv')\n",
        "df.info()\n",
        "\n",
        "import keras\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "EMBEDDING_DIM = 100\n",
        "#tokenizing data\n",
        "tokenizer =keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['tweet'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "#sequence creation\n",
        "X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
        "X =keras.preprocessing.sequence. pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)\n",
        "\n",
        "#one hot\n",
        "Y = pd.get_dummies(df['class']).values\n",
        "print('Shape of label tensor:', Y.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#splitting data to train and test\n",
        "X_train, X_test, Y_train, Y_test =train_test_split(X,Y, test_size = 0.30, random_state = 4)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding,SpatialDropout1D,LSTM,Bidirectional\n",
        "from keras.callbacks import  EarlyStopping\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
        "'''\n",
        "#model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu')) \n",
        "model.add(layers.Conv1D(128, 5, activation='relu')) \n",
        "model.add(layers.MaxPooling1D()) \n",
        "#model.add(layers.GlobalMaxPooling1D()) \n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(50, activation='relu')\n",
        "'''\n",
        "#model.add(50, activation='relu')\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 400\n",
        "\n",
        "#training\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)#,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
        "\n",
        "#prediction\n",
        "pred = model.predict_classes(X_train, verbose=1)\n",
        "\n",
        "#since y is one hot (this has to be performed)\n",
        "one_y = Y_train.argmax(-1)\n",
        "\n",
        "#Model Validation on train data set\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "print(confusion_matrix(one_y,pred))\n",
        "print(classification_report(one_y,pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}